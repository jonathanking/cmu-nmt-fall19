import torch


class SelfAttention(torch.nn.Module):
    """ Self-attn module for Transformer. """
    def __init__(self):
        super(SelfAttention, self).__init__()

class PositionwiseFeedForward(torch.nn.Module):
    """ Self-attn module for Transformer. """
    def __init__(self):
        super(PositionwiseFeedForward, self).__init__()

class PositionalEncoding(torch.nn.Module):
    """ Self-attn module for Transformer. """
    def __init__(self):
        super(PositionalEncoding, self).__init__()
